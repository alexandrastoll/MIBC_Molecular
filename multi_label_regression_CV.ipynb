{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import albumentations\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "import itertools\n",
    "import imblearn\n",
    "import timm\n",
    "\n",
    "##helper functions\n",
    "import import_ipynb\n",
    "from helper_functions import (getListOfFiles, strat_k_fold, sampler_strat_kfold, add_imgs_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Source: https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Multi_Point_Regression.ipynb, Access: 29.10.23.\n",
    "\n",
    "class ToListTensor(DisplayedTransform):\n",
    "    \"Transform to int tensor\"\n",
    "\n",
    "    _show_args = {'label': 'text'}\n",
    "    def __init__(self, split_idx=None,):\n",
    "        super().__init__(split_idx=split_idx)\n",
    "\n",
    "    def encodes(self, o): return o\n",
    "    def decodes(self, o): return TitledList(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchPredsCallback(Callback):\n",
    "    \n",
    "    \n",
    "    \"\"\"A callback to fetch and store predictions during the validation.\n",
    "    The parameter \"tiles_case\" needs to be specified here and in training loop both for.\"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    remove_on_fetch = True\n",
    "    def __init__(self, ds_idx=1, dl=None, with_input=False, with_decoded=False, cbs=None, reorder=True, tiles_case=50,\n",
    "                 epochs=2, run_n=0, split_n=0, table=True, regr=False):\n",
    "        self.cbs = L(cbs)\n",
    "        store_attr('ds_idx,dl,with_input,with_decoded,reorder')\n",
    "        self.cust_metrics = {\"epoch\" : [], \"acc\" : [], \"acc_agg\" : [], \"auc_macro\" : [],\n",
    "                             \"auc_macro_agg\" : []}\n",
    "        self.tiles_case=tiles_case\n",
    "        self.run_n = run_n\n",
    "        self.split_n = split_n\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.table = table\n",
    "        self.regr = regr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def after_validate(self):\n",
    "        \n",
    "        import import_ipynb\n",
    "        from helper_functions import myFunc\n",
    "        \n",
    "        to_rm = L(cb for cb in self.learn.cbs if getattr(cb, 'remove_on_fetch', False))\n",
    "        \n",
    "        if self.regr == True:\n",
    "            \n",
    "            with self.learn.removed_cbs(to_rm + self.cbs) as learn:\n",
    "                self.preds = learn.get_preds(ds_idx=self.ds_idx, dl=self.dl,\n",
    "                    with_input=self.with_input, with_decoded=self.with_decoded, with_loss=True, inner=True, reorder=self.reorder,\n",
    "                                            act=myFunc)\n",
    "            self.learn.val_y = torch.argmax(self.preds[1], dim=1).detach()\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            with self.learn.removed_cbs(to_rm + self.cbs) as learn:\n",
    "                        self.preds = learn.get_preds(ds_idx=self.ds_idx, dl=self.dl,\n",
    "                            with_input=self.with_input, with_decoded=self.with_decoded, with_loss=True, inner=True, reorder=self.reorder,\n",
    "                                            act=myFunc)\n",
    "                    \n",
    "            self.learn.val_y = self.preds[1].detach()\n",
    "        \n",
    "        self.learn.val_preds = torch.softmax(self.preds[0], dim=1).detach()\n",
    "        self.learn.val_logits = self.preds[0].detach()\n",
    "        self.learn.max_preds = torch.argmax(self.learn.val_preds, dim=1).numpy()  \n",
    "        \n",
    "        self.n_cases = len(self.learn.val_preds)/self.tiles_case \n",
    "        classes = self.learn.dls.c\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.learn.agg_probs = torch.softmax(torch.stack([(self.preds[0][self.tiles_case*i:self.tiles_case*(i+1)].mean(axis=0)) for i in range(int(self.n_cases))]), dim=1).detach()\n",
    "\n",
    "        self.learn.val_losses = self.preds[2].detach()                             \n",
    "\n",
    "            \n",
    "        self.learn.agg_preds = torch.argmax(self.learn.agg_probs, dim=1).detach()\n",
    "        \n",
    "        self.learn.agg_targs = torch.tensor([self.learn.val_y[self.tiles_case*i] for i in range(int(self.n_cases))]).detach()\n",
    "        self.learn.agg_losses = torch.stack([(self.learn.val_losses[self.tiles_case*i:self.tiles_case*(i+1)].mean(axis=0)) for i in range(int(self.n_cases))]).detach().numpy()\n",
    "        \n",
    "        acc = accuracy(self.learn.val_preds, self.learn.val_y).item()\n",
    "        self.acc = acc\n",
    "        \n",
    "        acc_agg = (self.learn.agg_preds == self.learn.agg_targs).float().mean().item()\n",
    "        self.acc_agg = acc_agg\n",
    "        \n",
    "        auc_macro = roc_auc_score(self.learn.val_y, self.learn.val_preds, multi_class=\"ovr\", average=\"macro\")\n",
    "        self.auc_macro = auc_macro\n",
    "        \n",
    "        auc_macro_agg = roc_auc_score(self.learn.agg_targs, self.learn.agg_probs.numpy(), multi_class=\"ovr\", average=\"macro\")\n",
    "        self.auc_macro_agg = auc_macro_agg\n",
    "        \n",
    "        if self.cust_metrics[\"epoch\"]:\n",
    "            self.cust_metrics[\"epoch\"].append(self.cust_metrics[\"epoch\"][-1]+1)\n",
    "        else:\n",
    "            self.cust_metrics[\"epoch\"].append(0)\n",
    "        self.cust_metrics[\"acc\"].append(self.acc)\n",
    "        self.cust_metrics[\"acc_agg\"].append(self.acc_agg)\n",
    "        self.cust_metrics[\"auc_macro\"].append(self.auc_macro)\n",
    "        self.cust_metrics[\"auc_macro_agg\"].append(self.auc_macro_agg)\n",
    "        self.learn.cust_metrics = self.cust_metrics\n",
    "        \n",
    "        print(f\"acc: {np.round(self.acc, 4)}, acc_agg: {np.round(self.acc_agg, 4)}, \"\n",
    "             f\"auc_macro: {np.round(self.auc_macro, 4)}, auc_macro_agg: {np.round(self.auc_macro_agg, 4)} \"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        if self.table == True and len(learn.cust_metrics[\"epoch\"]) >= self.epochs +1:\n",
    "        \n",
    "            for x in dls.valid_ds:\n",
    "                \n",
    "                if self.regr:\n",
    "                    \n",
    "                    self.labels.append(torch.argmax(x[1]).detach())\n",
    "                \n",
    "                else:\n",
    "                    self.labels.append(x[1].detach())\n",
    "                \n",
    "            self.labels = np.vstack(self.labels)\n",
    "\n",
    "            columns = [\"run_split\", \"img_path\", \"id_tile\", \"pred\", \"label\", \"loss\"]\n",
    "\n",
    "            VAL_TABLE_NAME = \"predictions\" \n",
    "\n",
    "            for a in dls.vocab:\n",
    "                columns.append(\"score_\" + a)\n",
    "            predictions_table_tile = wandb.Table(columns = columns)\n",
    "            \n",
    "            \n",
    "            ### log predicted and actual labels, and all scores\n",
    "            \n",
    "            for run_split, path, top_guess, scores, truth, loss in zip([f\"{self.run_n}.{self.split_n}\" for i in range(len(self.labels))],\n",
    "                                                                dls.valid_ds.items.iloc[:,0].reset_index(drop=True).tolist(),\n",
    "                                                               self.learn.max_preds, \n",
    "                                                               self.learn.val_logits.numpy(),\n",
    "                                                               self.labels,\n",
    "                                                               self.learn.val_losses):\n",
    "                img_path = re.sub(r'/[^/]*$', \"\", path)\n",
    "                img_id = re.search(r'/([^/]*)', path).group(1) \n",
    "                row = [run_split, img_path, img_id, dls.vocab[top_guess], dls.vocab[truth.item()], loss.item()]\n",
    "                \n",
    "                for s in scores.tolist():\n",
    "                    row.append(s)\n",
    "                predictions_table_tile.add_data(*row)\n",
    "\n",
    "            columns_agg = [\"run_split\", \"id_case\", \"pred_aggregated\", \"label\", \"loss_agg\"]\n",
    "\n",
    "            VAL_TABLE_NAME = \"predictions_agg\" \n",
    "\n",
    "            for a in dls.vocab:\n",
    "                columns_agg.append(\"score_\" + a)\n",
    "            predictions_table_agg = wandb.Table(columns = columns_agg)\n",
    "            \n",
    "            \n",
    "            ### log predicted and actual labels, and all scores\n",
    "            \n",
    "            df_cases = dls.valid_ds.items.iloc[[i for i in range(0, len(dls.valid_ds.items), self.tiles_case)]].reset_index(drop=True)\n",
    "            df_cases['Case ID'] = df_cases['Case ID'].apply(lambda x: x[:23])\n",
    "\n",
    "\n",
    "            for run_split, img_id, top_guess, scores, label, loss_agg in zip([f\"{self.run_n}.{self.split_n}\" for i in range(len(self.learn.agg_preds))],\n",
    "                                                                df_cases['Case ID'].tolist(), \n",
    "                                                               self.learn.agg_preds,\n",
    "                                                               self.learn.agg_probs.numpy(),\n",
    "                                                               df_cases.iloc[:,1].tolist(),\n",
    "                                                               self.learn.agg_losses):\n",
    "\n",
    "                row = [run_split, re.sub(r'/[^/]*$', \"\", img_id)[:-5], dls.vocab[top_guess], label, loss_agg]\n",
    "                for s in scores.tolist():\n",
    "                    row.append(s)\n",
    "                predictions_table_agg.add_data(*row)\n",
    "\n",
    "            self.learn.predictions_table_tile = predictions_table_tile\n",
    "            self.learn.predictions_table_agg = predictions_table_agg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training loop for repeated stratified k-fold cross-valdation with regression for 3 classes.\n",
    "\n",
    "tables_tile = []\n",
    "tables_agg_preds = []\n",
    "\n",
    "### df_tcga contains Case IDs with associated Consensus molecular subtypes and Pearson's Correlation values with the six Consensus subtypes for every Case ID.\n",
    "\n",
    "df_tcga = pd.read_excel(\"/path/to/file.xlsx\")\n",
    "df_tcga[\"Lum\"] = df_tcga[[\"LumNS\", \"LumP\", \"LumU\"]].max(axis=1)\n",
    "\n",
    "class_labels = [\"Lum\", \"Stroma-rich\", \"Ba/Sq\"]\n",
    "\n",
    "### df with case ids and corresponding simplified categorical labels (\"Lum\", \"Stroma-rich\", \"Ba/Sq\")\n",
    "df_train = pd.read_excel(\"/path/to/file.xlsx\")\n",
    "\n",
    "### filepath of subfolders containing patches, subfolders are termed after WSIs' names.\n",
    "base_path = \"/path/to/files/\"\n",
    "\n",
    "\n",
    "### making Albumentations compatible wit \"fast.ai\"\n",
    "\n",
    "### specifiying augmentations for training and validation.\n",
    "\n",
    "class AlbumentationsTransform(RandTransform):\n",
    "    split_idx,order=None,2\n",
    "    def __init__(self, train_aug, valid_aug): store_attr()\n",
    "    \n",
    "    def before_call(self, b, split_idx):\n",
    "        self.idx = split_idx\n",
    "    \n",
    "    def encodes(self, img: PILImage):\n",
    "        if self.idx == 0:\n",
    "            aug_img = self.train_aug(image=np.array(img))['image']\n",
    "        else:\n",
    "            aug_img = self.valid_aug(image=np.array(img))['image']\n",
    "        return PILImage.create(aug_img)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_train_aug(): return albumentations.Compose([\n",
    "             albumentations.HorizontalFlip(p=0.5),\n",
    "             albumentations.VerticalFlip(p=0.5),\n",
    "             albumentations.RandomRotate90(p=0.5),\n",
    "    ])\n",
    "\n",
    "def get_valid_aug(): return albumentations.Compose([\n",
    "    ])\n",
    "\n",
    "\n",
    "inputsize = Image.open(getListOfFiles(base_path)[0]).shape[0]\n",
    "epochs = 1\n",
    "stain_norm = 'macenko'\n",
    "runs = 20\n",
    "splits = 5\n",
    "\n",
    "### Apply normalization from Imagenet.\n",
    "batch_tfms = Normalize.from_stats(*imagenet_stats)\n",
    "\n",
    "\n",
    "\n",
    "### spefiying model name in \"\" calls models from the \"timm\" library. calling a model without \"\" calls the model from the \"fast.ai\" library.\n",
    "### add more paramaters for grid search.\n",
    "hyperparams = {\n",
    "    \"model\": [\"resnet18\", \"convnext_nano.in12k_ft_in1k\"],\n",
    "    \"w_decay\": [0.1],\n",
    "    \"base_lr\": [0.002],\n",
    "    \"batch_size\": [60],\n",
    "    \"rs\": [\"ros\"],\n",
    "    \"tiles_case\": [500]\n",
    "    }\n",
    "\n",
    "\n",
    "for i in tqdm(list(itertools.product(hyperparams[\"model\"], hyperparams[\"w_decay\"], hyperparams[\"base_lr\"], hyperparams[\"batch_size\"], hyperparams[\"rs\"], hyperparams[\"tiles_case\"]))):\n",
    "    model, w_decay, base_lr, batch_size, rs, tiles_case = i[0], i[1], i[2], i[3], i[4], i[5]\n",
    "    \n",
    "    tables_tile = []\n",
    "    tables_agg_preds = []\n",
    "\n",
    "        \n",
    "    if model == \"resnet18\":\n",
    "        resize = 224\n",
    "        \n",
    "    elif model == \"convnext_nano.in12k_ft_in1k\":\n",
    "        resize = 288\n",
    "    \n",
    "    print(f\"model: {model}, w_decay: {w_decay}, base_lr: {base_lr}, batch_size: {batch_size}, rs: {rs}, resize: {resize}, \"\n",
    "         f\"tiles_case: {tiles_case}\")\n",
    "    \n",
    "    item_tfms=[Resize(resize), AlbumentationsTransform(get_train_aug(), get_valid_aug())]\n",
    "    batch_tfms = Normalize.from_stats(*imagenet_stats)\n",
    "\n",
    "    ### 20 repetitions\n",
    "    \n",
    "    for z in range(runs): \n",
    "\n",
    "        dfs = sampler_strat_kfold(strat_k_fold(df_train, n_splits=splits, random_state=None), rs=rs, random_state=None, random_state_valid=None)\n",
    "        \n",
    "        dfs_train = [df[df.iloc[:,2] == False].reset_index(drop=True) for df in dfs]\n",
    "        dfs_valid = [df[df.iloc[:,2] == True].reset_index(drop=True) for df in dfs]\n",
    "        \n",
    "        \n",
    "        dfs_new_train = add_imgs_dataset(dfs_train, base_path, tiles_case=tiles_case, random_state=None)\n",
    "        dfs_new_valid = add_imgs_dataset(dfs_valid, base_path, tiles_case=tiles_case, random_state=None)\n",
    "        \n",
    "        dfs_new = [pd.concat([dfs_new_train[i], dfs_new_valid[i]]).reset_index(drop=True) for i in range(splits)]\n",
    "\n",
    "        ### 5 splits\n",
    "        \n",
    "        for i in range(splits): \n",
    "            \n",
    "            for label_df in class_labels:\n",
    "                dfs_new[i].insert(2, label_df, np.nan)\n",
    "            \n",
    "            for class_label in sorted(class_labels.copy(), reverse=True):\n",
    "                for index, row in dfs_new[i].iterrows():\n",
    "                    if row[\"Case ID\"][:12] in df_tcga[\"ID\"].tolist():\n",
    "                        dfs_new[i].loc[index, class_label] = df_tcga.loc[df_tcga[\"ID\"].tolist().index(row[\"Case ID\"][:12]), class_label]\n",
    "        \n",
    "            \n",
    "\n",
    "            dfs_new[i][\"combined\"] = dfs_new[i][class_labels].values.tolist()\n",
    "            \n",
    "            ### tracking for wandb.ai\n",
    "\n",
    "            group_name = (f\"{model}_{inputsize}_resize:{resize}_{rs}_tiles_:{tiles_case}\"\n",
    "                f\"{stain_norm}_epochs_ft:{epochs}_base_lr:{base_lr}_pt2_rotflip\")\n",
    "\n",
    "            wandb.init(settings=wandb.Settings(start_method=\"fork\"),\n",
    "                       project= \"project_name\",\n",
    "                    group = group_name,\n",
    "                    job_type=f\"run{z}\", save_code=True,\n",
    "                    config = {\"model\": model,\n",
    "                    \"input_size\": inputsize,\n",
    "                    \"resize\": resize,\n",
    "                    \"sampler\": rs,\n",
    "                    \"tiles\": tiles_case,\n",
    "                    \"stain_norm\": stain_norm,\n",
    "                    \"base_lr\": base_lr,\n",
    "                    \"epochs_fine_tune\": epochs,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"w_decay\": w_decay,\n",
    "                    \"item_tfms\": item_tfms,\n",
    "                    \"batch_tfms\": batch_tfms,\n",
    "                    \"group\": group_name,\n",
    "                    \"grid_search\": f\"w_decay: {w_decay}, base_lr: {base_lr}, batch_size: {batch_size}\"\n",
    "                    })   \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ### calling \"fast.ai\" API\n",
    "\n",
    "            datablock = DataBlock(blocks=(ImageBlock, RegressionBlock(n_out=3)),\n",
    "                            splitter=ColSplitter(col=f\"split{i}\"),\n",
    "                            get_x=ColReader(cols = 'Case ID', pref = base_path),\n",
    "                            get_y=Pipeline([ColReader(\"combined\"), ToListTensor]),\n",
    "                            item_tfms=item_tfms,\n",
    "                            batch_tfms=batch_tfms)\n",
    "            \n",
    "            \n",
    "            dls= datablock.dataloaders(dfs_new[i], bs = batch_size)\n",
    "            dls.vocab = [\"Lum\", \"Stroma-rich\", \"Ba/Sq\"]\n",
    "            learn = vision_learner(dls, model, metrics=[MSELossFlat()], loss_func=L1LossFlat(), y_range=(0,0.8),\n",
    "                        wd=w_decay, lr=base_lr, cbs=[FetchPredsCallback(tiles_case=tiles_case, epochs=epochs, regr=True,\n",
    "                        run_n=z, split_n=i), WandbCallback(log_dataset=False, log_model=False, log_preds=False)]).to_fp16()\n",
    "            \n",
    "            \n",
    "            ### training\n",
    "            \n",
    "            learn.fine_tune(epochs=epochs, base_lr=base_lr, wd=w_decay)\n",
    "            for metric in range(len(learn.cust_metrics[\"epoch\"])):\n",
    "\n",
    "                wandb.log(dict(zip(list(learn.cust_metrics.keys()), [item[metric] for item in learn.cust_metrics.values()])))\n",
    "\n",
    "            wandb.config.update({\"dir_run\": wandb.run.dir})\n",
    "            \n",
    "            \n",
    "            tables_tile.append(learn.predictions_table_tile)\n",
    "            tables_agg_preds.append(learn.predictions_table_agg)\n",
    "            \n",
    "            \n",
    "            del learn \n",
    "          \n",
    "        dir_wandb = wandb.run.dir\n",
    "        \n",
    "        if z+1 == runs:\n",
    "                 \n",
    "            ### storing final results + documents\n",
    "\n",
    "            tables_tile_compl = pd.concat([pd.DataFrame(tables_tile[i].data, columns=tables_tile[i].columns) for i in range(len(tables_tile))])\n",
    "            tables_agg_preds_compl = pd.concat([pd.DataFrame(tables_agg_preds[i].data, columns=tables_agg_preds[i].columns) for i in range(len(tables_agg_preds))])\n",
    "\n",
    "            sorted_last_3_cols_tile_compl = sorted(tables_tile_compl.columns[-3:])\n",
    "            sorted_cols_tile_compl = list(tables_tile_compl.columns[:-3]) + sorted_last_3_cols_tile_compl\n",
    "            tables_tile_compl = tables_tile_compl[sorted_cols_tile_compl]\n",
    "\n",
    "            sorted_last_3_cols_agg_preds_compl = sorted(tables_agg_preds_compl.columns[-3:])\n",
    "            sorted_tables_agg_preds_compl = list(tables_agg_preds_compl.columns[:-3]) + sorted_last_3_cols_agg_preds_compl\n",
    "            tables_agg_preds_compl = tables_agg_preds_compl[sorted_tables_agg_preds_compl]\n",
    "\n",
    "            tables_tile_compl.to_csv(dir_wandb+\"/\"+\"tiles_complete.csv\", sep='\\t', index=False)\n",
    "            tables_agg_preds_compl.to_csv(dir_wandb+\"/\"+\"agg_preds_complete.csv\", sep='\\t', index=False)\n",
    "\n",
    "            dic1_ = {k: np.mean([np.array(np.mean(tables_tile_compl[tables_tile_compl[\"id_tile\"].isin([j])].iloc[:,-3:], axis=0)) for j in tables_tile_compl[tables_tile_compl[\"img_path\"].isin([k])][\"id_tile\"].unique()], axis=0) for k in tables_tile_compl[\"img_path\"].unique()}\n",
    "\n",
    "            data = [(k, *v) for k, v in dic1_.items()]\n",
    "\n",
    "\n",
    "            df_agg_l = pd.DataFrame(data, columns=['case id'] + sorted(tables_agg_preds_compl.columns[-3:]))\n",
    "\n",
    "            df_agg_lab = pd.DataFrame.from_dict({\"case id\": tables_agg_preds_compl.id_case, \"label\": tables_agg_preds_compl.label}).drop_duplicates()\n",
    "            df_agg_lab = df_agg_lab.reset_index(drop=True)\n",
    "\n",
    "\n",
    "            wandb.log({\"auroc_agg_ensemble\": roc_auc_score(np.array(pd.get_dummies(df_agg_lab[\"label\"])), torch.softmax(torch.tensor(np.array(df_agg_l.iloc[:, -3:])), axis=1).numpy(), multi_class=\"ovr\", average=\"macro\")})\n",
    "            wandb.log({\"acc_agg_ensemble\": accuracy_score(np.argmax(np.array(pd.get_dummies(df_agg_lab[\"label\"])), axis=1), np.argmax(np.array(df_agg_l.iloc[:, -3:]), axis=1))})\n",
    "            df_agg_l.to_csv(dir_wandb+\"/\"+\"ensemble_agg_preds.csv\", sep='\\t', index=False)\n",
    "\n",
    "        else:     \n",
    "\n",
    "            wandb.finish()\n",
    "\n",
    "    time.sleep(0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3 fastai2",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
